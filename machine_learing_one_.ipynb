{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4VkufmNkUjdWGNsTqrgLN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wanjaperpetual/assignment/blob/main/machine_learing_one_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and prepare the MNIST dataset.\n",
        "The pixel values of the images range from 0\n",
        "through 255.\n",
        "**Scale these values to a range of 0 to 1 by dividing the values by 255.0.**\n",
        "his also converts the sample data from integers to floating-point numbers:\n",
        "\n",
        "> the sample data from integers to floating-point numbers:\n",
        "\n"
      ],
      "metadata": {
        "id": "2aCX92OqFfK3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxTV5w0wFKCW",
        "outputId": "414d9ca6-3bf6-486d-947e-195fc1778302"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.17.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8qODWpIFqBN",
        "outputId": "9a32e337-18ad-423b-d42b-4d856d2d8315"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "mnist.load_data() loads the MNIST dataset. It returns two tuples:\n",
        "\n",
        "(x_train, y_train) for the training set (images and corresponding labels).\n",
        "(x_test, y_test) for the test set (images and corresponding labels).\n",
        "\n",
        "\n",
        "x_train and x_test contain the images (28x28 pixel grayscale images of digits).\n",
        "\n",
        "y_train and y_test contain the labels (which digit the image represents, i.e., a number between 0 and 9)."
      ],
      "metadata": {
        "id": "g46mmajWHC1S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build a machine learning model"
      ],
      "metadata": {
        "id": "rqBM8Y1iHw6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(10)\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcV9sFGTGOqQ",
        "outputId": "0bfc7e75-90ae-41e7-e35b-db67e3ade233"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sequential Model This creates a sequential model, which means the layers are stacked one after the other\n",
        "\n",
        "The Flatten layer takes the 28x28 pixel image and flattens it into a 1D array (28x28 = 784 values).\n",
        "\n",
        "This is necessary because the next layers (Dense layers) expect a flat input rather than a 2D image matrix.\n",
        "input_shape=(28, 28) specifies the shape of the input image"
      ],
      "metadata": {
        "id": "AIU6bTYIMX2n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Flatten layer takes the 28x28 pixel image and flattens it into a 1D array (28x28 = 784 values).\n",
        "\n",
        "This is necessary because the next layers (Dense layers) expect a flat input rather than a 2D image matrix.\n",
        "\n",
        "input_shape=(28, 28) specifies the shape of the input image\n"
      ],
      "metadata": {
        "id": "nAeFg3sdNcCd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This layer randomly \"drops out\" (sets to 0) 20% of the neurons in the previous layer during training.\n"
      ],
      "metadata": {
        "id": "dGxUSUS7NjWA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ". Dense Layer (Output Layer with 10 units\n",
        "This is the output layer with 10 neurons, corresponding to the 10 possible classes (digits 0-9).\n",
        "Each neuron will output a score representing the model's confidence in each digit class.\n",
        "There's no activation function here because the next step is usually to apply softmax, which converts these scores into probabilities during training.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eQJFFCbZO1RG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model(x_train[:1]).numpy()\n",
        "predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a2_Pl63PBQm",
        "outputId": "050fc998-f9eb-4d36-caff-f43aa4797ec4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.20474295, -0.07953407,  0.3742419 , -0.88700193,  0.13990517,\n",
              "         0.371109  ,  0.908881  ,  0.3552382 , -0.0735388 , -0.0220452 ]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The tf.nn.softmax function converts these logits to probabilities for each class:**"
      ],
      "metadata": {
        "id": "nszSB9muX6k_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.nn.softmax(predictions).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6I7lmIjXqdj",
        "outputId": "257942f2-dacd-4497-c042-37a29df7c404"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.09871855, 0.07429145, 0.11695295, 0.03313294, 0.09252094,\n",
              "        0.11658712, 0.19961905, 0.1147514 , 0.07473817, 0.07868753]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "tf.keras.losses.SparseCategoricalCrossentropy:\n",
        "\n",
        "This is a built-in loss function in TensorFlow used for multi-class classification problems where the target labels are integers (class indices).\n",
        "Sparse categorical crossentropy is used instead of categorical crossentropy when the labels are provided as integers (e.g., 0, 1, 2, ..., n-1 for n classes), making it memory efficient because it avoids one-hot encoding of the labels.\n",
        "from_logits=True:\n",
        "\n",
        "This parameter indicates that the output of the model consists of logits (raw scores) rather than probabilities.\n",
        "When from_logits=True is set, the function will automatically apply the softmax activation function internally to convert the logits to probabilities before calculating the loss. This is important because softmax helps in interpreting the raw scores as probabilities"
      ],
      "metadata": {
        "id": "-wqR8Uwb5MKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "hXxHWNfdYwBK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "y_train[:1]: This is the actual label (true target) for the first sample in the training set. Since you're using sparse categorical crossentropy, this label is represented as an integer, indicating the class (digit 0-9) that the first sample belongs to.\n",
        "\n",
        "predictions: This contains the raw logits (unprocessed scores) output by the model for the first sample. Each of the 10 values in predictions corresponds to one of the digit classes (0-9), and these logits are not yet normalized into probabilities.\n",
        "\n",
        "loss_fn(y_train[:1], predictions): This computes the sparse categorical cross-entropy loss between the true label (y_train[:1]) and the raw logits (predictions).\n",
        "\n",
        "Since from_logits=True, the loss function will apply softmax internally to the logits, converting them to probabilities before calculating the loss.\n",
        "The result is a scalar value representing how \"wrong\" the model’s prediction was for the first sample. The lower the loss, the better the prediction matches the true labe"
      ],
      "metadata": {
        "id": "On7kXAdx5czZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn(y_train[:1], predictions).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5HqBnZdZHlq",
        "outputId": "a3fa47a0-f264-4340-fca9-6b503728e1c5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.1491165"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before you start training, configure and compile the model using Keras Model.compile. Set the optimizer class to adam, set the loss to the loss_fn function you defined earlier, and specify a metric to be evaluated for the model by setting the metrics parameter to accuracy"
      ],
      "metadata": {
        "id": "DLGg5cK16HUN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "optimizer='adam':\n",
        "\n",
        "The Adam optimizer is a popular optimization algorithm that adapts the learning rate for each parameter in the model.\n",
        "loss=loss_fn:\n",
        "\n",
        "This sets the loss function used for training. In this case, loss_fn was\n",
        "metrics=['accuracy']:\n",
        "\n",
        "This specifies that the model will track accuracy during training and evaluation.\n",
        "\n",
        "After compiling the model, when you call model.fit() to train the model, the following steps happen:\n",
        "\n",
        "Optimization: The Adam optimizer updates the model’s parameters to minimize the loss.\n",
        "Loss Calculation: The sparse categorical cross-entropy loss is computed to evaluate how well the model’s predictions match the true labels.\n",
        "\n",
        "Metrics Calculation: In each epoch, accuracy is calculated to provide insight into how well the model is performing at classifying the digits correctly."
      ],
      "metadata": {
        "id": "xJT9v1tv6-In"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=loss_fn,\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "hsUVmMoX6F8W"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train and evaluate your model**\n",
        "\n",
        "Use the Model.fit method to adjust your model parameters and minimize the los\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ud-my9l16Lbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9g2-VOVY7gGD",
        "outputId": "172fe0f0-6c23-4e54-bd56-f42557e15eb7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.8656 - loss: 0.4717\n",
            "Epoch 2/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - accuracy: 0.9551 - loss: 0.1520\n",
            "Epoch 3/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.9672 - loss: 0.1096\n",
            "Epoch 4/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9725 - loss: 0.0910\n",
            "Epoch 5/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.9763 - loss: 0.0751\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7f503a163d90>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and Learning:\n",
        "\n",
        "During each epoch, the model adjusts its weights based on the loss and the gradients computed from the training data.\n",
        "\n",
        "The goal is to minimize the loss and improve the model’s performance on the training set.\n",
        "\n",
        "Training typically involves multiple epochs because a single pass over the dataset is usually insufficient for the model to learn effectively.\n",
        "\n",
        "Batch Processing:\n",
        "\n",
        "During training, the dataset is often split into batches (groups of samples) that are passed through the network in one go.\n",
        "\n",
        "Each time a batch is passed through the model, the weights are updated. One epoch is complete when all batches have been processed.\n",
        "\n",
        "Overfitting and Underfitting:\n",
        "\n",
        "Too few epochs: The model may not have learned enough from the data, leading to underfitting.\n",
        "\n",
        "Too many epochs: The model might start to memorize the training data instead of generalizing to unseen data, leading to overfitting. M\n",
        "\n",
        "onitoring the validation loss/accuracy can help determine when to stop training"
      ],
      "metadata": {
        "id": "0YvKgPne72r9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An epoch refers to one complete pass of the entire training dataset through the neural network during the training process"
      ],
      "metadata": {
        "id": "mpsrXRAC72SO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "s used to evaluate the trained model on a test dataset. It returns the loss value and any other metrics (such as accuracy) that you have specified when compiling the model.\n",
        "\n",
        "Explanation:\n",
        "\n",
        "x_test: This is the test dataset's input features (in the case of MNIST, it would be images of digits).\n",
        "\n",
        "y_test: These are the true labels (target outputs) for the test dataset (the digit classes corresponding to the images in x_test).\n",
        "\n",
        "verbose=2: Controls the verbosity level of the output:\n",
        "\n",
        "verbose=0: No output will be printed.\n",
        "\n",
        "verbose=1: A progress bar is displayed for each batch during evaluation.\n",
        "\n",
        "verbose=2: A more compact, one-line summary of the evaluation results is printed."
      ],
      "metadata": {
        "id": "9pdcBllI88MI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(x_test,  y_test, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59uXcRYe8NyD",
        "outputId": "304fd8c0-557a-4d52-e3f3-acc02a49fb3d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 - 1s - 2ms/step - accuracy: 0.9736 - loss: 0.0790\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.07898963242769241, 0.9735999703407288]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want your model to return a probability, you can wrap the trained model, and attach the softmax to it\n",
        "\n",
        "Softmax is an activation function commonly used in the output layer of a neural network for multi-class classification problems. It converts raw logits (unprocessed scores) into probabilities, where each probability represents the likelihood of the input belonging to a particular class.\n",
        "\n",
        "Key Properties:\n",
        "The softmax function squashes the output of each class into a range between 0 and 1.\n",
        "The sum of all output probabilities is 1, making it ideal for classification tasks where the goal is to assign a class label to the input."
      ],
      "metadata": {
        "id": "BIiZOs9i9L1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "probability_model = tf.keras.Sequential([\n",
        "  model,\n",
        "  tf.keras.layers.Softmax()\n",
        "])"
      ],
      "metadata": {
        "id": "Kj42XZ7X9M7W"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probability_model(x_test[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkp5dDMW9h7s",
        "outputId": "8ad3c5b5-943c-4dfc-f14c-cc5e40009224"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5, 10), dtype=float32, numpy=\n",
              "array([[7.6012151e-07, 1.2137344e-09, 5.8612272e-06, 5.4495427e-04,\n",
              "        1.6575603e-10, 1.5939419e-06, 5.6553801e-14, 9.9938595e-01,\n",
              "        3.9325298e-07, 6.0498580e-05],\n",
              "       [6.8780852e-09, 8.9505775e-06, 9.9997532e-01, 1.5618796e-05,\n",
              "        3.5931680e-14, 8.8104080e-08, 1.7298192e-09, 8.6058860e-12,\n",
              "        2.4655716e-08, 5.5449731e-13],\n",
              "       [4.5905045e-07, 9.9836773e-01, 7.0653768e-04, 7.5473305e-05,\n",
              "        9.7490018e-05, 5.2874620e-06, 4.4635541e-05, 4.6686374e-04,\n",
              "        2.3328899e-04, 2.1829344e-06],\n",
              "       [9.9976844e-01, 2.3651201e-09, 6.3225001e-05, 5.8514761e-08,\n",
              "        1.4860895e-07, 1.4877529e-05, 1.3944253e-04, 9.3795343e-06,\n",
              "        8.8649594e-09, 4.4848362e-06],\n",
              "       [5.4169399e-05, 3.5773917e-09, 2.3314383e-04, 3.5825269e-07,\n",
              "        9.9387276e-01, 2.1449199e-05, 2.3130224e-05, 1.6415313e-04,\n",
              "        2.8098921e-06, 5.6280009e-03]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}